{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"name":"eda-prophet-winning-solution-3-0.ipynb","provenance":[]}},"cells":[{"cell_type":"code","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true,"colab":{"base_uri":"https://localhost:8080/"},"id":"68j1iKiZTXjL","executionInfo":{"status":"ok","timestamp":1617862219779,"user_tz":-480,"elapsed":5503,"user":{"displayName":"Manav Arora","photoUrl":"","userId":"15200164079717752008"}},"outputId":"f1efcdc2-a9c4-444e-bc86-222b23f778a5"},"source":["import warnings\n","\n","with warnings.catch_warnings():\n","    warnings.simplefilter(\"ignore\")\n","    import numpy as np\n","    import pandas as pd\n","    from collections import OrderedDict\n","    import re\n","    import statsmodels\n","    from statsmodels.nonparametric.smoothers_lowess import lowess\n","    import statsmodels.api as sm\n","#     import sklearn\n","    import fbprophet\n","    from fbprophet import Prophet\n","#     import pymc3\n","#     import lightgbm as lgb\n","#     import xgboost as xgb\n","#     import tensorflow as tf\n","    import os\n","    import datetime as dt\n","    import matplotlib.pyplot as plt\n","#     import seaborn as sns\n","    import bokeh\n","    from bokeh.models import CustomJS, ColumnDataSource, Slider, Label, Div, HoverTool, Band, Span, BoxAnnotation\n","    from bokeh.plotting import figure\n","    from bokeh.palettes import Spectral11\n","    import ipywidgets as widgets\n","    from IPython.display import display\n","    from typing import Union, Dict, List, Callable\n","    from contextlib import contextmanager\n","    import sys, os\n","    import datetime as dt\n","\n","print('numpy version: ', np.__version__)\n","print('pandas version: ', pd.__version__)\n","print('statsmodels version: ', statsmodels.__version__)\n","print('prophet version: ', fbprophet.__version__)\n","# print('xgboost version: ', xgb.__version__)\n","# print('pymc3 version: ', pymc3.__version__)\n","# print('tensorflow version: ', tf.__version__)\n","print('ipywidgets version: ', widgets.__version__)\n","warnings.filterwarnings('ignore', module='matplotlib')\n","bokeh.io.output_notebook()"],"execution_count":1,"outputs":[{"output_type":"stream","text":["numpy version:  1.19.5\n","pandas version:  1.1.5\n","statsmodels version:  0.10.2\n","prophet version:  0.7.1\n","ipywidgets version:  7.6.3\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vgNi6hnWG6Xa","executionInfo":{"status":"ok","timestamp":1617862378567,"user_tz":-480,"elapsed":42076,"user":{"displayName":"Manav Arora","photoUrl":"","userId":"15200164079717752008"}},"outputId":"1195ea51-164d-4e8a-94e1-5ec6d21ebf17"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"USUbgCxpG7ET","executionInfo":{"status":"ok","timestamp":1617862396119,"user_tz":-480,"elapsed":1831,"user":{"displayName":"Manav Arora","photoUrl":"","userId":"15200164079717752008"}},"outputId":"c983fd90-f4fb-4127-88ad-2413691f7319"},"source":["%cd /content/drive/MyDrive/CZ4041 Machine Learning Project/Colab/data"],"execution_count":3,"outputs":[{"output_type":"stream","text":["/content/drive/.shortcut-targets-by-id/1ldppAeVJfaoI4cvwZyTCju9jXbD4LI-H/CZ4041 Machine Learning Project/Colab/data\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"e2c2a9ded212aca161c50f1f26494b980a4af313","id":"NbNBdYy3TXjN"},"source":["# timer\n","# console output supresses\n","\n","class timer_gen:\n","    \"\"\"Simple timer\"\"\"\n","    \n","    def __init__(self):\n","        self.t0 = dt.datetime.now()\n","        self.t1, self.t2 = None, self.t0\n","    def __iter__(self):\n","        return self\n","    def __next__(self):\n","        self.t1, self.t2 = self.t2, dt.datetime.now()\n","        return \"<timer = {} ({})>\".format(self.t2 - self.t1, self.t2 - self.t0)\n","\n","@contextmanager\n","def suppress_stdout(on=True):\n","    \"\"\"Supress console output\"\"\"\n","    \n","    if on:\n","        with open(os.devnull, \"w\") as devnull:\n","            old_stdout = sys.stdout\n","            sys.stdout = devnull\n","            try:  \n","                yield\n","            finally:\n","                sys.stdout = old_stdout\n","    else:\n","        yield\n","\n","class suppress_stdout_stderr(object):\n","    \"\"\"Supress console output 2.0\"\"\"\n","    \n","    def __init__(self):\n","        # Open a pair of null files\n","        self.null_fds = [os.open(os.devnull, os.O_RDWR) for x in range(2)]\n","        # Save the actual stdout (1) and stderr (2) file descriptors.\n","        self.save_fds = (os.dup(1), os.dup(2))\n","\n","    def __enter__(self):\n","        # Assign the null pointers to stdout and stderr.\n","        os.dup2(self.null_fds[0], 1)\n","        os.dup2(self.null_fds[1], 2)\n","\n","    def __exit__(self, *_):\n","        # Re-assign the real stdout/stderr back to (1) and (2)\n","        os.dup2(self.save_fds[0], 1)\n","        os.dup2(self.save_fds[1], 2)\n","        # Close the null files\n","        os.close(self.null_fds[0])\n","        os.close(self.null_fds[1])\n","        \n","from IPython.display import display_html\n","def display_side_by_side(*args):\n","    html_str=''\n","    for df in args:\n","        if type(df) == pd.Series:\n","            df = pd.DataFrame(df, columns=['value'])\n","        html_str+=df.to_html()\n","    display_html(html_str.replace('table','table style=\"display:inline\"'),raw=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true,"_kg_hide-input":true,"id":"ppHZtFZVTXjO"},"source":["print(\"Working directory: \", os.getcwd())\n","print(\"Input directory: \", os.path.abspath(\"../input\"))\n","print(\"Input data: \", os.listdir(\"../input\"))\n","print(os.listdir(\"../input/sales-prophet-cv-2017\"))\n","\n","df_train = pd.read_csv('../input/demand-forecasting-kernels-only/train.csv', parse_dates=['date'])\n","df_test = pd.read_csv('../input/demand-forecasting-kernels-only/test.csv', parse_dates=['date'])\n","df_train.sales = df_train.sales.astype(np.float)\n","display_side_by_side(df_train.head(3), df_test.head(3))\n","print('Entries (Train / Test) : {} / {}'.format(len(df_train), len(df_test)))\n","s_train, s_test = df_train.store.unique(), df_test.store.unique()\n","print('Stores (Train / Test) : {} - {} / {} - {}'.format(s_train[0], s_train[-1], s_test[0], s_test[-1]))\n","s_train, s_test = df_train.item.unique(), df_test.item.unique()\n","print('Items (Train / Test) : {} - {} / {} - {}'.format(s_train[0], s_train[-1], s_test[0], s_test[-1]))\n","dates_train, dates_test = df_train.date.unique(), df_test.date.unique()\n","print('Dates (Train / Test) : {:.10} - {:.10} / {:.10} - {:.10}'.format(dates_train[0], dates_train[-1], dates_test[0], dates_test[-1]))\n","display(pd.concat([df_train.isnull().sum().rename('Training NaNs'),\n","                   df_train.isnull().sum().rename('Test NaNs')], axis=1))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"trusted":true,"_uuid":"00d6fe28feed6d576ab565512a6eff9191230a10","scrolled":true,"_kg_hide-input":true,"_kg_hide-output":false,"id":"OyA6Xdr2TXjP"},"source":["# --- Matplot + Ipywidgets\n","\n","# %matplotlib notebook\n","# %matplotlib inline\n","\n","def update_ts_simple(s1_num=1, s2_num=2, i1_num=1, i2_num=2):\n","    fig, ax = plt.subplots(4, figsize = (12, 8))\n","    df_train.query('store == @s1_num & item == @i1_num').set_index('date')['sales'].resample('W').mean().plot(ax = ax[0])\n","    df_train.query('store == @s1_num & item == @i2_num').set_index('date')['sales'].resample('W').mean().plot(ax = ax[1])\n","    df_train.query('store == @s1_num & item == @i1_num').set_index('date')['sales'].resample('W').mean().plot(ax = ax[2])\n","    df_train.query('store == @s2_num & item == @i2_num').set_index('date')['sales'].resample('W').mean().plot(ax = ax[3])\n","    ax[0].set_title('item {} store {}'.format(i1_num, s1_num))\n","    ax[1].set_title('item {} store {}'.format(i2_num, s1_num))\n","    ax[2].set_title('item {} store {}'.format(i1_num, s2_num))\n","    ax[3].set_title('item {} store {}'.format(i2_num, s2_num))\n","    fig.suptitle('Sales Volume TS (Ipywidgets)')\n","    fig.tight_layout(rect=[0, 0, 1, 0.94])\n","    fig.canvas.draw()\n","    fig.show()\n","    \n","s1_slider = widgets.IntSlider(value=1, min=1, max=10, continuous_update=False, description='store A', layout={'width': '2.1in', 'height': '1in'})\n","s2_slider = widgets.IntSlider(value=2, min=1, max=10, continuous_update=False, description='store B', layout={'width': '2.1in', 'height': '1in'})\n","i1_slider = widgets.IntSlider(value=1, min=1, max=50, continuous_update=False, description='item A', layout={'width': '2.1in', 'height': '1in'})\n","i2_slider = widgets.IntSlider(value=2, min=1, max=50, continuous_update=False, description='item B', layout={'width': '2.1in', 'height': '1in'})\n","ui = widgets.HBox([s1_slider, s2_slider, i1_slider, i2_slider], layout={'min_width': '12in'})\n","out = widgets.interactive_output(update_ts_simple, {'s1_num': s1_slider, 's2_num': s2_slider, 'i1_num': i1_slider, 'i2_num': i2_slider})\n","display(ui, out)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"trusted":true,"_uuid":"88476281683509416f2512f3b5ef42122e8be390","scrolled":false,"_kg_hide-input":true,"id":"kxVT-n8STXjQ"},"source":["# --- Matplot + Ipywidgets\n","# --- Should be a efficient but does NOT work\n","\n","# store_num = 1\n","# a, b = 1, 10\n","# sales = df_train.loc[(df_train.store == store_num), ['date', 'sales', 'item']]\n","# sales = sales.pivot(index='date', columns='item', values='sales')\n","# sales.columns = ['it_{}'.format(x) for x in sales.columns]\n","# sales_w = sales.resample('W').sum()\n","# # display(sales_w.head(3))\n","# source_data = ColumnDataSource(data=sales_w)\n","# p1 = figure(plot_width=750, plot_height=150, title='item {}'.format(a), x_axis_type='datetime', tools=\"pan,box_zoom,reset\")\n","# line1 = p1.line(x='date', y='it_{}'.format(a), source=source_data)\n","# p2 = figure(plot_width=750, plot_height=150, title='item {}'.format(b), x_axis_type='datetime', tools=p1.tools,\n","#             x_range=pa.x_range)\n","# p2.line('date', 'it_{}'.format(b), source=source_data)\n","\n","# def update(w):\n","#     linea.glyph.line_width = w\n","#     linea.glyph.y = 'it_{}'.format(w)\n","#     bokeh.io.push_notebook()\n","# bokeh.io.show(column(p1, p2), notebook_handle=True)\n","# wid = widgets.interactive(update, w=(1,50))\n","# wid.children[0].description = \"\"\n","# display(wid)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"trusted":true,"_uuid":"5fae8084145fdb3476f4d51bacc623f75abb03ac","_kg_hide-input":true,"scrolled":true,"id":"41GyAJdhTXjQ"},"source":["# --- Pure Bokeh\n","\n","i1, i2 = '1_1', '2_1'\n","df_train['it_sa'] = df_train.item.astype(str) + '_' + df_train.store.astype(str) \n","sales = df_train.pivot(index='date', columns='it_sa', values='sales').resample('W').mean()\n","df_train.drop(columns=['it_sa'], inplace=True)\n","# display(sales_w.head(3))\n","sales_source = sales.loc[:, [i1, i2]].copy()\n","source = ColumnDataSource(data=sales_source)\n","source_ref = ColumnDataSource(data=sales)\n","p1 = figure(plot_width=750, plot_height=150, title=i1, x_axis_type='datetime', tools=\"pan,wheel_zoom,reset\")\n","line1 = p1.line(x='date', y=i1, source=source)\n","p2 = figure(plot_width=750, plot_height=150, title=i2, x_axis_type='datetime', tools=p1.tools,\n","            x_range=p1.x_range)\n","line2 = p2.line('date', i2, source=source)\n","p1.add_tools(HoverTool(tooltips=[('sales', '@{}'.format(i1)), ('vs.', '@{}'.format(i2))], \n","                       renderers=[line1, line2], mode='vline'))\n","p2.add_tools(HoverTool(tooltips=[('sales', '@{}'.format(i2)), ('vs.', '@{}'.format(i1))], \n","                       renderers=[line1, line2], mode='vline'))\n","\n","slider_it1 = Slider(start=1, end=50, value=1, step=1, title=\"item a\", callback_policy=\"mouseup\")\n","slider_it2 = Slider(start=1, end=50, value=1, step=1, title=\"item b\")\n","slider_sa1 = Slider(start=1, end=10, value=1, step=1, title=\"store a\")\n","slider_sa2 = Slider(start=1, end=10, value=1, step=1, title=\"store b\")\n","js_code = \"\"\"\n","    var v = cb_obj.value;\n","    var y_old = source.data['{old}'];\n","    var y_new = ref.data[{new}];\n","    for (var i = 0; i < y_old.length; i++) {\n","        y_old[i] = y_new[i];\n","    }\n","    source.change.emit();\n","\"\"\"\n","callback_it1 = CustomJS(args=dict(source=source, ref=source_ref, s=slider_sa1), code=js_code.replace('{old}', i1).replace('{new}', 'v + \"_\" + s.value'))\n","callback_it2 = CustomJS(args=dict(source=source, ref=source_ref, s=slider_sa2), code=js_code.replace('{old}', i2).replace('{new}', 'v + \"_\" + s.value'))\n","callback_sa1 = CustomJS(args=dict(source=source, ref=source_ref, s=slider_it1), code=js_code.replace('{old}', i1).replace('{new}', 's.value + \"_\" + v'))\n","callback_sa2 = CustomJS(args=dict(source=source, ref=source_ref, s=slider_it2), code=js_code.replace('{old}', i2).replace('{new}', 's.value + \"_\" + v'))\n","slider_it1.js_on_change('value', callback_it1)\n","slider_it2.js_on_change('value', callback_it2)\n","slider_sa1.js_on_change('value', callback_sa1)\n","slider_sa2.js_on_change('value', callback_sa2)\n","\n","layout = bokeh.layouts.column(Div(text='<h4>Sales Volumes TS (Bokeh)</h4>'),\n","                              bokeh.layouts.row(slider_it1, slider_sa1), p1, \n","                              bokeh.layouts.row(slider_it2, slider_sa2), p2)\n","bokeh.io.show(layout)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true,"_uuid":"45d0b2801d70875e3778a8f4de29a64aa740f505","id":"TwpnaQ7zTXjR"},"source":["# --- Bokeh + Ipywidgets\n","# --- Probably a simpler way to update data source in a Boken plot\n","\n","# x = np.linspace(0, 2*np.pi, 2000)\n","# y = np.sin(x)\n","\n","# p = figure(title=\"simple line example\", plot_height=300, plot_width=600, y_range=(-5,5))\n","# r = p.line(x, y, color=\"#2222aa\", line_width=3)\n","\n","# def update(f, w=1, A=1, phi=0):\n","#     if   f == \"sin\": func = np.sin\n","#     elif f == \"cos\": func = np.cos\n","#     elif f == \"tan\": func = np.tan\n","#     r.data_source.data['y'] = A * func(w * x + phi)\n","#     bokeh.io.push_notebook()\n","    \n","# bokeh.io.show(p, notebook_handle=True)\n","# widgets.interact(update, f=[\"sin\", \"cos\", \"tan\"], w=(0,100), A=(1,5), phi=(0, 20, 0.1))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"c9ce18d5254df07fee2e6877df1b421719d77653","scrolled":false,"id":"V7aMZpeSTXjR"},"source":["def add_datepart(df, fldname, inplace=False, drop=False):\n","    if not inplace: df = df.copy()        \n","    fld = df[fldname]\n","    fld_dtype = fld.dtype\n","    if isinstance(fld_dtype, pd.core.dtypes.dtypes.DatetimeTZDtype):\n","        fld_dtype = np.datetime64\n","    if not np.issubdtype(fld_dtype, np.datetime64):\n","        df[fldname] = fld = pd.to_datetime(fld, infer_datetime_format=True)\n","    targ_pre = re.sub('[Dd]ate$', '', fldname)\n","    \n","    attr = ['Year', 'Month', 'Week', 'Day', 'Dayofweek', 'Dayofyear','Weekofyear']\n","#     attr = ['Year', 'Month', 'Week', 'Day', 'Dayofweek', 'Dayofyear','Weekofyear',\n","#             'Is_month_end', 'Is_month_start', 'Is_quarter_end', 'Is_quarter_start', 'Is_year_end', 'Is_year_start']\n","    for n in attr: \n","        df[targ_pre + n] = getattr(fld.dt, n.lower())\n","    if drop: \n","        df.drop(fldname, axis=1, inplace=True)\n","    if not inplace: return df \n","\n","df_trainext = add_datepart(df_train, 'date', inplace=False)\n","# df_testext = add_datepart(df_test, 'date', inplace=False)\n","display(df_trainext.head(3))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"25a0b1f56e15650bd919e83ea58227e324897037","scrolled":false,"id":"C7WgTHKUTXjS"},"source":["df_trainext.groupby('date').mean()['sales'].plot(figsize=(12,3), title='Sales TS (aggregated data)')\n","\n","fig, ax = plt.subplots(2, 2, figsize=(10, 10))\n","_ = pd.pivot_table(df_trainext, values='sales', columns='Year', index='Month').plot(title=\"Yearly seasonality\", ax=ax[0,0])\n","_ = pd.pivot_table(df_trainext, values='sales', columns='Month', index='Day').plot(title=\"Monthly seasonality\", ax=ax[0,1])\n","_ = pd.pivot_table(df_trainext, values='sales', columns='Year', index='Dayofweek').plot(title=\"Weekly seasonality (by year)\", ax=ax[1,0])\n","_ = pd.pivot_table(df_trainext, values='sales', columns='Month', index='Dayofweek').plot(title=\"Weekly seasonality (by month)\", ax=ax[1,1])\n","fig.suptitle('Sales seasonality patterns (aggregated data)')\n","fig.tight_layout(rect=[0, 0, 1, 0.96])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"trusted":true,"_uuid":"ef89e5397907ba2352e8d3d9d949d0235e69f57d","id":"dmSAIBSSTXjS"},"source":["_ = pd.pivot_table(df_trainext, values='sales', index='Year').plot(style='-o', title=\"Annual trend (aggregated data)\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"trusted":true,"_uuid":"8bbb4aed60fb786a6e7074d31bfbd8944aae2a86","id":"msrFfL18TXjT"},"source":["df_train_norm = df_trainext.copy()\n","df_train_norm['sales'] /= df_trainext.groupby(['item', 'store'])['sales'].transform('mean')\n","_ = df_train_norm.groupby(['date'])['sales'].std().plot(figsize=(12,3), title='Volatility (across items and stores)')\n","_ = (df_train_norm.groupby(['store', 'item'])[['date', 'sales']].rolling(30, on='date').std().groupby(['date']).mean()\n","     .plot(figsize=(12,3), title='Volatility (30-d rolling, aggregated data)'))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"trusted":true,"_uuid":"e365cf61c91ad6074c83b09c1a6919d3caf7f3b2","_kg_hide-input":true,"scrolled":true,"id":"KX7UYUodTXjT"},"source":["freq_season_mapping = {'None':None, 'Weekly': 7, 'Monthly':30, 'Yearly':365}\n","\n","def update_stl_decompose(i_num, s_num, seasonality='Yearly', stl_style='additive'):\n","    ts = df_train.query('store == @s_num & item == @i_num').set_index('date')['sales']\n","    freq = freq_season_mapping[seasonality]\n","    \n","    fig, ax = plt.subplots(5, 1, figsize=(12,10))\n","    decomposition = sm.tsa.seasonal_decompose(ts, model=stl_style, freq=freq)\n","    _ = decomposition.observed.plot(ax=ax[0], title='observed')\n","    _ = decomposition.trend.plot(ax=ax[1], title='trend')\n","    _ = decomposition.seasonal.plot(ax=ax[2], title='seasonal')\n","    _ = decomposition.resid.plot(ax=ax[3], title='residual')\n","    res = decomposition.resid.values\n","    res = res[np.isfinite(res)]\n","#     adfuller_stat = statsmodels.tsa.stattools.adfuller(res)\n","    ljungbox_stat = statsmodels.stats.diagnostic.acorr_ljungbox(res)\n","    statsmodels.graphics.tsaplots.plot_pacf(res, ax=ax[4], lags=40, \n","                                           title='residuals pacf; ljung-box p-value = {:.2E} / {:.2E}'.format(ljungbox_stat[1][6], \n","                                                                                                              ljungbox_stat[1][30]))\n","    fig.suptitle('STL decomposition')\n","    fig.tight_layout(rect=[0, 0, 1, 0.96])\n","    \n","s_slider = widgets.IntSlider(min=1, max=10, continuous_update=False, description='store', layout={'width': '2.1in', 'height': '1in'})\n","i_slider = widgets.IntSlider(min=1, max=50, continuous_update=False, description='item', layout={'width': '2.1in', 'height': '1in'})\n","season_drop = widgets.Dropdown(value='Yearly', options=['Weekly', 'Monthly', 'Yearly'], description='seasonality', layout={'width': '2.1in'})\n","stltype_drop = widgets.Dropdown(value='multiplicative', options=['additive', 'multiplicative'], description='STL type', layout={'width': '2.1in'})\n","ui = widgets.HBox([s_slider, i_slider, season_drop, stltype_drop], layout={'min_width': '6in', 'max_width': '6in'})\n","out = widgets.interactive_output(update_stl_decompose, {'s_num': s_slider, 'i_num': i_slider, \n","                                                        'seasonality': season_drop, 'stl_style': stltype_drop})\n","display(ui, out)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"_uuid":"7881dccdb28aadcf30cf8cbe304ab5e4c54476a3","id":"SDgboSg1TXjT"},"source":["<a id=\"SectionProphet\"></a>\n","# Prophet"]},{"cell_type":"markdown","metadata":{"_uuid":"8b5501039877155e9716570b18b16179d0d4acd0","id":"dex4f0jRTXjU"},"source":["Now, let's test Facebook Prophet package. Like statsmodels STL, Prophet also uses a trend-seasonality decomposition; however, it is more flexible and allows making predictions. Under the hood, Prophet solves a state space model using the Bayesian framework of [Stan](http://mc-stan.org/).\n","\n","**Pros:**\n","- Quite easy to use\n","- Allows specifying multiple seasonalities\n","- Allows specifying special events\n","- Can compute quick MAP and slow but accurate Bayesian estimates\n","- Provides methods for basic plotting out of the box\n","\n","**Cons:**\n","- Can only treat univariate time series\n","- Assumes Gaussian priors\n","- Does not provide methods to tune hyper-parameters out of the box (for example, seasonality and trend flexibility priors)\n","- Outputs useless warning messages in the console - and I found no way to hide them"]},{"cell_type":"markdown","metadata":{"_uuid":"7c52fea6864afbf019001f72ce4999a5c722e8e0","id":"ZWJa0BdyTXjU"},"source":["## First look\n","\n","Prophet here is fit on 2013 - 2016 data with annual and weekly seasonalities using additive decomposition. The latest version of the package allows selecting multiplicative decomposition, we don't have it on Kaggle. Alas! Our analysis above has shown that it might have been beneficial. Of course the keen ones can mannualy log-transform the data to obtain a similar effect.\n","\n","In any case, the fit looks reasonable. Trend and seasonalities of sales across stores and items look similar and consistent with our previous analysis. We see that Prophet adjusted the trend quite a bit in 2014 - well, we saw the trend in 2014 was kind of an outiler on the aggregate level. We see almost zero uncertainty in the trend component, which is probably good (uncertainty in the seasonal component is not plotted). \n","\n","Partial autocorrelation plot of the predicted residuals looks much better than the one for STL decomposition. Ljung-Box tests is doing definitely better as well; p-value for monthly lags is a bit low but still acceptable.\n","\n","I also output SMAPE over time for the training (2013 - 2016) and validation (2017 Q1) data, smoothed using LOWESS for better visibility. On average, SMAPE is around 15 and 16 for the training and validation data, respectively. Of course, we yet have no idea how it would perform on the real test data (2018 Q1). \n","\n","Overall, the results look okay."]},{"cell_type":"code","metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"7b53aa87d4d41c9e78e609624bc1a1616edcaf8f","_kg_hide-output":true,"scrolled":true,"id":"GsXk6mRnTXjU"},"source":["# SMAPE - the official metric for the submission\n","\n","def smape(y: Union[np.ndarray, float], yhat: Union[np.ndarray, float], average=True, signed=False) -> float:\n","    \"\"\"SMAPE evaluation metric\"\"\"\n","    \n","    if signed:\n","        result = 2. * (yhat - y) / (np.abs(y) + np.abs(yhat)) * 100\n","    else:\n","        result = 2. * np.abs(yhat - y) / (np.abs(y) + np.abs(yhat)) * 100\n","    if average: return np.mean(result)\n","    return result\n","\n","def smape_df(df: pd.DataFrame, average=True, signed=False) -> pd.DataFrame:\n","    return smape(df.y, df.yhat, average=average, signed=signed)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"trusted":true,"_uuid":"eb0580be2a47a877cf9aa73b0e79a300c7001913","_kg_hide-input":true,"scrolled":false,"id":"N0SYMYJdTXjU"},"source":["# -- TODO: Add Ipywidgets?\n","\n","def prophet_show(item, store, cutoff_train, cutoff_eval, prophet_kwargs, title, \n","                 plot_components=True, display_df=True):\n","    ts = (df_train.query('item == @item & store == @store & date < @cutoff_eval')[['date', 'sales']]\n","          .rename(columns={'date':'ds', 'sales':'y'})).reset_index(drop=True)\n","    ind_train = pd.eval('ts.ds < cutoff_train')\n","    ind_eval = ~ ind_train\n","    len_train, len_eval = ind_train.sum(), ind_eval.sum()\n","    ts_train = ts.loc[ind_train]\n","    m = Prophet(**prophet_kwargs)\n","    m.fit(ts_train)\n","    ts_hat = m.predict(ts).merge(ts[['ds', 'y']], on='ds', how='left')\n","    if display_df: display(ts_hat.tail(3))\n","\n","    df_combined = ts_hat.assign(smape=0, smape_smooth=0)\n","    df_combined.smape = smape_df(df_combined, average=False)\n","    df_combined.loc[ind_train, 'smape_smooth'] = lowess(df_combined.loc[ind_train, 'smape'], range(len_train), frac=0.03, return_sorted=False)\n","    df_combined.loc[ind_eval, 'smape_smooth'] = lowess(df_combined.loc[ind_eval, 'smape'], range(len_eval), frac=0.35, return_sorted=False)\n","    smape_in = df_combined.loc[ind_train].smape.mean()\n","    smape_oos = df_combined.loc[ind_eval].smape.mean()\n","    \n","    source = ColumnDataSource(data=df_combined)\n","    p = figure(plot_width=750, plot_height=200, title=(\"**{}**     item = {} store = {}     train / test = ..{} / ..{}\"\n","                                                       .format(title, item, store, cutoff_train, cutoff_eval)), \n","               x_axis_type='datetime', tools=\"pan,wheel_zoom,reset\")\n","    _ = p.line(x='ds', y='yhat', source=source)\n","    _ = p.line(x='ds', y='yhat_lower', source=source, line_alpha=0.4)\n","    _ = p.line(x='ds', y='yhat_upper', source=source, line_alpha=0.4)\n","    _ = p.scatter(x='ds', y='y', source=source, color='black', radius=0.2, radius_dimension='y', alpha=0.4)\n","    _ = p.scatter(x='ds', y='y', source=source, color='black', radius=0.2, radius_dimension='y', alpha=0.4)\n","       \n","    deltas = np.abs(m.params['delta'][0])\n","    delta_max = np.max(deltas)\n","    df_deltas = pd.DataFrame({'ds': m.changepoints.values, 'delta':deltas, 'delta_scaled':ts_hat.yhat.mean() * deltas / delta_max})\n","    source2 = ColumnDataSource(df_deltas)\n","    cp1 = p.vbar(x='ds', source=source2, width=1, top=ts_hat.yhat.mean(), color='red', alpha=0.2, hover_color='red', hover_alpha=1)\n","    cp2 = p.vbar(x='ds', source=source2, width=1.5e+9, top='delta_scaled', color='red', alpha=0.5)\n","    p.add_tools(HoverTool(tooltips=[('trend delta', '@delta{.000}')], renderers=[cp2], mode='mouse'))\n","    # p.add_layout(Label(x=1e+10, y=10, text='xasfdfsdfsd'))\n","    p.add_layout(BoxAnnotation(left=ts_train.ds.iloc[-1], right=ts.ds.iloc[-1]))\n","    \n","    p2 = figure(plot_width=750, plot_height=100, title=\"SMAPE IS / OOS = {:.3f} / {:.3f}\".format(smape_in, smape_oos), x_axis_type='datetime', tools=\"\",\n","                x_range=p.x_range)\n","    sm1 = p2.line(x='ds', y='smape_smooth', source=source, color='green')\n","    p2.add_tools(HoverTool(tooltips=[('smape', '@smape')], renderers=[sm1], mode='vline', line_policy='interp'))\n","    p2.add_layout(BoxAnnotation(left=ts_train.ds.iloc[-1], right=ts.ds.iloc[-1]))\n","    p2.yaxis[0].ticker.desired_num_ticks = 2\n","    bokeh.io.show(bokeh.layouts.column(p, p2))\n","    \n","    if plot_components:\n","        _ = m.plot_components(ts_hat, uncertainty=True)\n","        fig, ax = plt.subplots(1, 1, figsize=(12, 2))\n","#         res = ts_hat.query('ds < @cutoff_train').yhat - ts_train.y\n","        res = (df_combined['y'] - df_combined['yhat'])\n","#         adfuller_stat = statsmodels.tsa.stattools.adfuller(res.values)\n","        ljungbox_stat = statsmodels.stats.diagnostic.acorr_ljungbox(res.values)\n","        _ = statsmodels.graphics.tsaplots.plot_pacf(res, lags=40, ax=ax,\n","                                                    title='residuals pacf; ljung-box p-value = {:.2E} / {:.2E}'.format(ljungbox_stat[1][6], \n","                                                                                                                      ljungbox_stat[1][30]))\n","    \n","prophet_show(item=1, store=1, cutoff_train=\"2017-01-01\", cutoff_eval=\"2017-04-01\",\n","             prophet_kwargs={'yearly_seasonality':True, 'weekly_seasonality':True,\n","                            'uncertainty_samples':500},\n","            title='Prophet')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"_uuid":"ecaa5d53806ac8b971d7fba993a0c35ec6ab87f2","id":"u3BLp8USTXjV"},"source":["## Cross-validated error\n","\n","Now let's properly evaluate Prophet predictive strenght using cross-validation (CV). It is always tricky to cross-validate a time series. Here I used the following strategy: a model is fit on the data since the day 0 till the day $X_k$, then the model is evaluated on the data from the day $X_k$ till the day $X_{k+1}$. The plot below shows CV SMAPE evaluated on quaterly CV folds, ranging from 2015 Q1 to 2017 Q4.\n","\n","While in-sample SMAPE is quite stable, out-of-sample SMAPE varies quite a lot: it ranges from 11 to 18. Unfortunately, the worst performace is always in the first quarter of the year - and evaluation period for the submission is 2018 Q1."]},{"cell_type":"code","metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"3bf091491886e8de58e69dd3dc169de72161294f","id":"vVhh8mrITXjV"},"source":["prophet_cv = pd.read_csv('../input/sales-prophet-cv-2017/cv_prophet_yk.csv', index_col=[0,1,2,3])\n","display(prophet_cv.head(3))\n","print('N rows = ', prophet_cv.shape[0])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"trusted":true,"_uuid":"85e9ebda9f51a422c2998e28126e99375fdf0b4b","_kg_hide-input":true,"scrolled":true,"id":"N4hVBlifTXjV"},"source":["def show_cv_sampe_agg(data_cv):\n","    cv_error = data_cv.groupby(['cv_fold', 'sample']).apply(smape_df)\n","    display(pd.DataFrame(cv_error).T)\n","    source_index = cv_error[:, 'oos'].index\n","    source_in = ColumnDataSource(data=pd.DataFrame(cv_error[:, 'in'],\n","                                                   index=source_index,\n","                                                   columns=['smape']))\n","    source_oos = ColumnDataSource(data=pd.DataFrame(cv_error[:, 'oos'], \n","                                                   index=source_index,\n","                                                   columns=['smape']))\n","    p = figure(plot_width=750, plot_height=250, title=\"Prophet CV SMAPE\", \n","               x_range=source_index.values, tools=\"pan,wheel_zoom,reset\")\n","    p.xaxis.major_label_orientation = -np.pi / 4\n","    l1 = p.circle(x='cv_fold', y='smape', source=source_in, legend='in-sample', size=7)\n","    l2 = p.circle(x='cv_fold', y='smape', source=source_oos, color='red', legend='out-of-sample', size=6)\n","    p.legend.click_policy=\"hide\"\n","    p.add_tools(HoverTool(tooltips=[('smape', '@smape')], renderers=[l1, l2], mode='mouse'))\n","    bokeh.io.show(p)\n","    \n","show_cv_sampe_agg(prophet_cv)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"_uuid":"c9b6fe666ce569eb5ba3a2eaea5f943f9a0d2ef0","id":"5GgRbkjnTXjV"},"source":["If we take a closer look at one CV fold, we can see a clear monthly seasonal pattern for both in- and out-of-sample fit.\n","\n","We saw that sales volatility is higher in winter - so worse in-sample fit in winter is to be expected. High monthly osciliations out-of-sample are more difficult to interpret. There a number of possible explanations: whether we could not fit monthly seasonality properly, or we overfit yearly seasonality, or our trend does not generalize well, or it is once again due to osciliations in volatility...\n"," \n","Overall, however, there are no obvious red flags."]},{"cell_type":"code","metadata":{"trusted":true,"_uuid":"7cdcadac3d84c67b6c8c9acc33705986207750ec","_kg_hide-input":true,"scrolled":true,"id":"biWqNk09TXjV"},"source":["def show_cv_smape_by_time(data_cv, cutoff_eval):\n","    df = pd.DataFrame({'smape_in': data_cv.query('cv_fold == @cutoff_eval & sample == \"in\"').groupby(['ds']).apply(smape_df),\n","                       'smape_oos': data_cv.query('cv_fold == @cutoff_eval & sample == \"oos\"').groupby(['ds']).apply(smape_df)\n","                       })\n","    df['smape_in_smooth'] = lowess(df.smape_in, range(df.shape[0]), frac=0.015, return_sorted=False)\n","    df['smape_oos_smooth'] = lowess(df.smape_oos, range(df.shape[0]), frac=0.15, return_sorted=False)\n","    df.index = pd.to_datetime(df.index)\n","\n","    source = ColumnDataSource(df)\n","    p = figure(plot_width=750, plot_height=200, title=\"Prophet SMAPE by time: {} cv fold\".format(cutoff_eval),\n","               x_axis_type='datetime', tools=\"pan,wheel_zoom,reset\")\n","    sm1 = p.line(x='ds', y='smape_in_smooth', source=source, color='green')\n","    sm2 = p.line(x='ds', y='smape_oos_smooth', source=source, color='red')\n","    p.add_tools(HoverTool(tooltips=[('smape', '@smape_in')], renderers=[sm1], mode='vline', line_policy='interp'))\n","    p.add_tools(HoverTool(tooltips=[('smape', '@smape_oos')], renderers=[sm2], mode='vline', line_policy='interp'))\n","    # p.yaxis[0].ticker.desired_num_ticks = 2\n","    bokeh.io.show(p)\n","    \n","show_cv_smape_by_time(prophet_cv, cutoff_eval=\"2017-04-01\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"_uuid":"f6b803d82cb7c69f0d9551ddb296c55039b5740d","id":"_KF2utTgTXjW"},"source":["Let's check out-of-sample SMAPE only. The OOS error also has a clear repetitive pattern: in particular, 2015 Q1 kind of resembes 2016 Q1 and 2017 Q1.\n","\n","What does it tell us? It is yet another indication that our data is quite regular. Apart from that - not much really, but it may become more useful when compared to other models."]},{"cell_type":"code","metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"3d6f01ab9dccc9ff70c2638564186bd108566603","id":"1zO_yeiRTXjW"},"source":["def show_cv_smape_by_time_oos(data_cv):\n","    df = data_cv.query('sample == \"oos\" & ds > \"2015-01-01\"').groupby(['cv_fold', 'ds']).apply(smape_df).unstack(level='cv_fold')\n","#     folds = df['cv_fold'].unique()\n","#     df['smape_smooth'] = np.nan\n","    for col in df.columns:\n","        df[col] = lowess(df[col], range(df.shape[0]), frac=0.15, return_sorted=False)\n","    df.index = pd.to_datetime(df.index)\n","\n","    source = ColumnDataSource(df)\n","    p = figure(plot_width=750, plot_height=200, title=\"Prophet OOS SMAPE by time\",\n","               x_axis_type='datetime', tools=\"pan,wheel_zoom,reset\")\n","    for col in df.columns.values:\n","        _ = p.line(x='ds', y=col, source=source, color='red')\n","        p.add_layout(Span(location=pd.to_datetime(col).value / 1e6,\n","                          dimension='height', line_color='black',\n","                          line_dash='dashed', line_width=1))\n","    for col in np.append(df.columns.values, \"2015-01-01\"):\n","        p.add_layout(Span(location=pd.to_datetime(col).value / 1e6,\n","                          dimension='height', line_color='black',\n","                          line_dash='dashed', line_width=1))\n","    # p.yaxis[0].ticker.desired_num_ticks = 2\n","    p.add_tools(HoverTool(tooltips=[('smape', '$y')], mode='vline', line_policy='interp'))\n","    bokeh.io.show(p)\n","    \n","show_cv_smape_by_time_oos(prophet_cv)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"_uuid":"3e99e08bf86351bd66e8dde26462d489b9da4f9a","id":"vdGmXZrdTXjW"},"source":["Let's check what item-store combinations correspond to the best and worst predictions (i.e., predictions with the lowest and the highest SMAPE, respectively). \n","\n","We see that \"best performers\" have a smoother shape of their historical sales, while \"worst performers\" are generally noisier - well, nothing unexpected. Interestingly, all best performers have higher sales volume values. It means that noise in sales data is not purely multiplicative, which is consistent with our volatility plots."]},{"cell_type":"code","metadata":{"_kg_hide-input":true,"trusted":true,"scrolled":true,"_uuid":"b2b1697b4cc81293e89d78a8c9c941e6eb7c3f64","_kg_hide-output":false,"id":"IDaPZVyaTXjW"},"source":["def plot_data_cv_example(df, cv_fold_idx, num_idx, ax):\n","        ts = df.loc[(cv_fold_idx, 'in', num_idx, slice(None)), 'y'].reset_index(level=[0,1,2], drop=True)\n","        ts = ts[pd.notnull(ts)]\n","        ts.index = pd.to_datetime(ts.index)\n","        ax.plot(ts.resample('w').mean())\n","        ax.set_title(num_idx)\n","\n","def show_cv_best_worst(data_cv, cutoff_eval):\n","    cv_error_num = data_cv.groupby(['sample', 'num']).apply(smape_df)\n","    cv_error_num_oos = cv_error_num.loc['oos', :].sort_values()\n","    cv_error_num_in = cv_error_num.loc['in', :].sort_values()\n","    print(\"-------- Best: Lowest SMAPE ---------\")\n","    display_side_by_side(cv_error_num_in.head(), cv_error_num_oos.head())\n","    print(\"-------- Worst: Highest SMAPE ---------\")\n","    display_side_by_side(cv_error_num_in.tail(), cv_error_num_oos.tail())\n","\n","    cv_fold_idx = cutoff_eval\n","    fig, ax = plt.subplots(3, 1, figsize=(10, 5))\n","    plot_data_cv_example(df=data_cv, cv_fold_idx=cv_fold_idx, num_idx=cv_error_num_oos.index[0][1], ax=ax[0])\n","    plot_data_cv_example(df=data_cv, cv_fold_idx=cv_fold_idx, num_idx=cv_error_num_oos.index[1][1], ax=ax[1])\n","    plot_data_cv_example(df=data_cv, cv_fold_idx=cv_fold_idx, num_idx=cv_error_num_oos.index[2][1], ax=ax[2])\n","    fig.suptitle('Best: Lowest OOS SMAPE')\n","    fig.tight_layout(rect=[0, 0, 1, 0.94])\n","\n","    fig, ax = plt.subplots(3, 1, figsize=(10, 5))\n","    plot_data_cv_example(df=data_cv, cv_fold_idx=cv_fold_idx, num_idx=cv_error_num_oos.index[-1][1], ax=ax[0])\n","    plot_data_cv_example(df=data_cv, cv_fold_idx=cv_fold_idx, num_idx=cv_error_num_oos.index[-2][1], ax=ax[1])\n","    plot_data_cv_example(df=data_cv, cv_fold_idx=cv_fold_idx, num_idx=cv_error_num_oos.index[-3][1], ax=ax[2])\n","    fig.suptitle('Worst: Highest data_cv SMAPE')\n","    fig.tight_layout(rect=[0, 0, 1, 0.94])\n","    \n","show_cv_best_worst(prophet_cv, cutoff_eval=\"2017-04-01\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"_uuid":"9313fe391616d4d0ce16e194dd1eb16c46fbc0d8","id":"vQF1nsN5TXjX"},"source":["## Addings US holidays\n","\n","Prophet can incorporate information about holidays or any other special events. Splendid! Let's try to include US federal holidays. We don't know what these sales are but there is a good chance that the firm is US based or at least have US customers.\n","\n","Unfortunately, including US federal holidays does not seem to improve the fit. On the bright side, now I know when the birthday of Martin Luther King is!"]},{"cell_type":"code","metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"dcb44b8634e77a3c100c7574fffc524986428ec0","id":"QYUCUocaTXjX"},"source":["holidays_us_raw = pd.read_csv('../input/federal-holidays-usa-19662020/usholidays.csv')\n","display(holidays_us_raw.head(3))\n","holidays_us = pd.DataFrame({\n","    'holiday':'US',\n","    'ds':holidays_us_raw.Date, \n","    'lower_window': -1,\n","    'upper_window': 0})\n","display(holidays_us.head(3))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"bf39ec6a0350d32cb98e316560d14384f8db3bfc","scrolled":false,"id":"i8PCK_PzTXjX"},"source":["prophet_show(item=1, store=1, cutoff_train=\"2017-01-01\", cutoff_eval=\"2017-04-01\",\n","             prophet_kwargs={'yearly_seasonality':True, 'weekly_seasonality':True,\n","                             'holidays':holidays_us, 'uncertainty_samples':500},\n","            title='Prophet (US holidays)', plot_components=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"_uuid":"fc2ff0f28761201c78163459cb1fccf822da2ba4","id":"kRt_1KJMTXjX"},"source":["## Tuning hyper-parameters by cross-validation\n","\n","So far, we used Prophet with the default hyper-parameters that define seasonal and trend flexibility. This model gives a public score of 16.9 and a private score of 14.1.\n","\n","I tuned the hyper-parameters by cross-validation on 2017 Q1 in a separate notebook. The resulting model has a less flexible yearly seasonality and does not account for US holidays. That model gives a public score of 15.4 and a private score of 13.9. That's better... but the performance is still far from top models. At the moment of testing Prophet, top public scores had already been below 14.0.\n","\n","So, what do we do now? We could of course work a little more on tuning Prophet hyper-parameters. Instead, we'll try a new approach."]},{"cell_type":"code","metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"560676241a46ab1df4c18a5e4ff9604ad86d45c8","scrolled":false,"id":"K4Eka3gPTXjX"},"source":["# -- TODO: add CV plots \n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7J9wxELPF0J9"},"source":["#LGBM"]},{"cell_type":"code","metadata":{"id":"b4c5jVgFFy8K"},"source":["# importing necessary packages\n","import numpy as np \n","import pandas as pd \n","import lightgbm as lgb\n","import statsmodels.api as sm\n","import matplotlib.pyplot as plt\n","from sklearn.metrics import mean_absolute_error\n","from sklearn.model_selection import train_test_split\n","\n","\n","# defining some functions\n","def smape(x, x_):\n","    \"\"\"Return the smape value for two arrays.\"\"\"\n","    return 100 * np.mean(2 * np.abs(x - x_)/(np.abs(x) + np.abs(x_)))\n","    \n","def linear_fit_slope(y):\n","    \"\"\"Return the slope of a linear fit to a series.\"\"\"\n","    y_pure = y.dropna()\n","    length = len(y_pure)\n","    x = np.arange(0, length)\n","    slope, intercept = np.polyfit(x, y_pure.values, deg=1)\n","    return slope\n","\n","def linear_fit_intercept(y):\n","    \"\"\"Return the intercept of a linear fit to a series.\"\"\"\n","    y_pure = y.dropna()\n","    length = len(y_pure)\n","    x = np.arange(0, length)\n","    slope, intercept = np.polyfit(x, y_pure.values, deg=1)\n","    return intercept\n","\n","# importing input datasets\n","train = pd.read_csv('../input/train.csv', parse_dates=True, index_col=['date'])\n","test = pd.read_csv('../input/test.csv', parse_dates=True, index_col=['date'])\n","\n","# concatenating train and test data frames\n","test['sales'] = np.nan\n","df = pd.concat([train, test.loc[:, ['store', 'item', 'sales']]]).sort_values(by=['store', 'item'])\n","\n","# adding some time-related factors\n","df['quarter'] = df.index.quarter\n","df['month'] = df.index.month\n","df['dow'] = df.index.weekday\n","df['week_block_num'] = [int(x) for x in np.floor((df.index - pd.to_datetime('2012-12-31')).days/7) + 1]\n","df['quarter_block_num'] = (df.index.year - 2013) * 4 + df['quarter']\n","\n","\n","# detecting and handling outliers\n","\n","# finding the slope of a linear fit for sale values grouped by store, item and day of week\n","lin_slope_df = df.groupby(['store', 'item', 'dow'])['sales'].apply(linear_fit_slope).reset_index()\n","lin_slope_df.columns = ['store', 'item', 'dow', 'lin_slope']\n","df = df.reset_index().merge(lin_slope_df, how='left', on=['store', 'item', 'dow']).set_index('date')\n","\n","# finding the intercept of a linear fit for sale values grouped by store, item and day of week\n","lin_intercept_df = df.groupby(['store', 'item', 'dow'])['sales'].apply(linear_fit_intercept).reset_index()\n","lin_intercept_df.columns = ['store', 'item', 'dow', 'lin_intercept']\n","df = df.reset_index().merge(lin_intercept_df, how='left', on=['store', 'item', 'dow']).set_index('date')\n","\n","# fitting a linear model to the sale values grouped by store, item and day of week (trend)\n","df['linear_fit'] = (df['week_block_num'] - 1) * df['lin_slope'] + df['lin_intercept']\n","\n","# removing the increasing trend from the sale values\n","df['trend_removed_sales'] = df['sales'] - df['linear_fit']\n","\n","# removing the yearly seasonality from the sale values\n","differenced_df = df.groupby(['store', 'item', 'dow'])['trend_removed_sales'].rolling(window=53, min_periods=53).\\\n","apply(lambda x: x[-1]-x[0]).reset_index()\n","differenced_df.columns = ['store', 'item', 'dow', 'date', 'diff']\n","differenced_df = differenced_df.sort_values(by=['store', 'item', 'date'])\n","df['diff'] = differenced_df['diff'].values\n","\n","# normalizing the stationary sale values\n","diff_mean_df = df.groupby(['store', 'item', 'dow'])['diff'].mean().reset_index()\n","diff_mean_df.columns = ['store', 'item', 'dow', 'diff_mean']\n","df = df.reset_index().merge(diff_mean_df, how='left', on=['store', 'item', 'dow']).set_index('date')\n","\n","diff_std_df = df.groupby(['store', 'item', 'dow'])['diff'].std().reset_index()\n","diff_std_df.columns = ['store', 'item', 'dow', 'diff_std']\n","df = df.reset_index().merge(diff_std_df, how='left', on=['store', 'item', 'dow']).set_index('date')\n","\n","df['norm_diff'] = (df['diff'] - df['diff_mean']) / df['diff_std']\n","\n","# identifying outliers\n","df['outlier'] = (abs(df['norm_diff']) > 3) * 1\n","\n","# handling outliers (interpolation)\n","corrected_sales = []\n","for ind, row in df[df['outlier']==1].iterrows():\n","    past_week = ind - pd.Timedelta('7 days')\n","    next_week = ind + pd.Timedelta('7 days')\n","    store = row['store']\n","    item = row['item']\n","    past_week_sales = df.loc[past_week][(df.loc[past_week]['store']==store) & \n","    (df.loc[past_week]['item']==item)]['sales'].values[0]\n","    next_week_sales = df.loc[next_week][(df.loc[next_week]['store']==store) & \n","    (df.loc[next_week]['item']==item)]['sales'].values[0]\n","    interpolated_sales = 0.5 * (past_week_sales + next_week_sales)\n","    corrected_sales.append(interpolated_sales)\n","df.loc[df['outlier']==1, 'sales'] = corrected_sales\n","\n","# removing useless columns\n","df.drop(columns = ['linear_fit',\n","                   'lin_slope', \n","                   'lin_intercept', \n","                   'trend_removed_sales', \n","                   'diff', \n","                   'diff_mean', \n","                   'diff_std', \n","                   'norm_diff', \n","                   'outlier'], \n","                   inplace=True)\n","                 \n","# sorting dataframe\n","df = df.sort_values(by=['item', 'store'])\n","\n","# building expanding mean sale values for grouped by store, item, (day of week, month, and quarter)\n","# shift(1) allows to exclude the most recent sale value\n","for item in ['dow', 'month', 'quarter']:\n","    grouped_df = df.groupby(['store', 'item', item])['sales'].expanding().mean().shift(1).bfill().\\\n","    reset_index()\n","    grouped_df.columns = ['store', 'item', item,'date', item + '_ex_avg_sale']\n","    grouped_df = grouped_df.sort_values(by=['item', 'store', 'date'])\n","    df[item + '_ex_avg_sale'] = grouped_df[item + '_ex_avg_sale'].values\n","    \n","# finding store,items whose mean sales value is below the 50% percentile\n","# (later, the prediction for year 2018 for these store,items will be multiplied by a factor smaller than one)\n","store_item_mean_sale_series = df.groupby(['store', 'item'])['sales'].mean()\n","sale_critical_value = store_item_mean_sale_series.quantile(0.5)\n","critical_store_item_df = store_item_mean_sale_series[store_item_mean_sale_series < sale_critical_value].\\\n","reset_index()\n","critical_store_item_df.drop(columns=['sales'], inplace=True)\n","critical_store_item_df['critical_store_item'] = 1\n","df = df.reset_index().merge(critical_store_item_df, how='left', on=['store', 'item']).\\\n","set_index('date')\n","df['critical_store_item'] = df['critical_store_item'].fillna(0)\n","\n","# sorting dataframe\n","df = df.sort_values(by=['item', 'store'])\n","\n","# defining useful features for lightGBM\n","used_feats = ['month_ex_avg_sale', 'dow_ex_avg_sale', 'quarter_ex_avg_sale',\n","              'month', 'dow', 'quarter_block_num', 'critical_store_item', 'sales']\n","df = df.loc[:, used_feats]\n","\n","# defining training and testing data frames\n","training_df = df.loc['2013':'2017']\n","testing_df = df['2018']\n","\n","# defining a condition for critical store, items and droping the 'critical_store_item' value\n","critical_store_item_mask = testing_df['critical_store_item']==1\n","training_df.drop(columns=['critical_store_item'], inplace=True)\n","testing_df.drop(columns=['critical_store_item'], inplace=True)\n","\n","# developing a lightGBM model, evaluating it and investigating feature importance\n","X = training_df.loc[:, [col for col in training_df.columns if col not in ['sales']]]\n","y = training_df['sales']\n","X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=0)\n","lgb_train = lgb.Dataset(X_train, y_train)\n","lgb_val = lgb.Dataset(X_val, y_val, reference=lgb_train)\n","\n","# defining the parameters and hyper-parameters. For better result, the hyper-parameters should be \n","# tuned carefully.\n","params = {\n","    'task': 'train',\n","    'boosting_type': 'gbdt',\n","    'objective': 'regression',\n","    'num_leaves': 10,\n","    'max_depth': 3,\n","    'metric' : 'mape',\n","    'learning_rate': 0.1\n","}\n","gbm = lgb.train(params,\n","                lgb_train,\n","                num_boost_round=10000,\n","                valid_sets=lgb_val,\n","                early_stopping_rounds=50,\n","                verbose_eval=2000)\n","\n","preds = gbm.predict(X_val, num_iteration=gbm.best_iteration)\n","\n","print('validation smape: ', smape(y_val, preds))\n","print('validation mae: ', mean_absolute_error(y_val, preds))\n","\n","# investigating the distribution of the error\n","error = y_val.values - preds\n","\n","plt.figure(figsize=(15,5))\n","plt.subplot(1, 2, 1)\n","plt.hist(error, EDGECOLOR='black', color='y')\n","\n","# comparing the distribution of the predictin and the actual \n","sm.qqplot_2samples(y_val.values, preds, line='45', ax=plt.subplot(1, 2, 2))\n","plt.show()\n","\n","# exploring the feature importance\n","lgb.plot_importance(gbm, height=0.6)\n","plt.show()\n","\n","\n","# predicting sale values for year 2018\n","X_train = training_df.loc[:, [col for col in training_df.columns if col not in ['sales']]].values \n","y_train = training_df['sales'].values\n","X_test = testing_df.loc[:, [col for col in testing_df.columns if col not in ['sales']]]\n","lgb_train = lgb.Dataset(X_train, y_train)\n","\n","# defining the parameters and hyper-parameters. For better result, the hyper-parameters should be \n","# tuned carefully.\n","params = {\n","    'task': 'train',\n","    'boosting_type': 'gbdt',\n","    'objective': 'regression',\n","    'num_leaves': 10,\n","    'max_depth': 3,\n","    'learning_rate': 0.1\n","}\n","\n","gbm = lgb.train(params,\n","                lgb_train,\n","                num_boost_round=10000)\n","test_preds = gbm.predict(X_test, num_iteration=gbm.best_iteration)\n","\n","# adding prediction to testing dataframe\n","testing_df.loc[:,'sales'] = test_preds\n","\n","# for critical store items, the prediction is multplied by a factor slightly smaller than 1 as it \n","# apears the model overpredicts them\n","testing_df.loc[critical_store_item_mask, 'sales'] = testing_df.loc[critical_store_item_mask, 'sales'] * 0.99\n","\n","# creating submission\n","sample_submission = pd.read_csv('../input/sample_submission.csv')\n","sample_submission['sales'] = testing_df['sales'].values\n","sample_submission.to_csv('submision_1.csv',index=False)\n","print(sample_submission.head())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"_uuid":"f5a087cfba25ab60238410c3e5b5d78e50f3e1d0","id":"Y-Bi2ybxTXjX"},"source":["<a id=\"SectionDumb\"></a>\n","# The winning \"dumb\" solution"]},{"cell_type":"markdown","metadata":{"_uuid":"7120db49f29037afca3274ce3e524f5e00982fe5","id":"jBr_Qfe0TXjY"},"source":["Let's get back to our data analysis. If you remember, we've seen quite a few quirks:\n","- The data has no apperent outliers at all;\n","- Visually, shapes of sales volume time series across all items and stores look suspiciously regular and similar to each other;\n","- Weekly and yearly seasonal patterns are very stable for every store-item combination, and trend is relatively stable too;\n","- Consistency between time series level and standard deviation indicates unusual regularity in time series noise component;\n","- Very simple state space models fit the data quite well, which indicates regularity in time series trend and seasonal components;\n","- Out-of-sample error has a seasonal pattern, which may indicate some structural regularity in sales time series.\n","\n","Okay, we cannot ignore this anymore. This sales volume data is way too clean and regular... it's totally synthetic! To be honest, I was not very happy when I found out: that was not a real-world problem I had hoped to solve... Well, two chocolate bars managed to make me feel better.\n","\n","Short time after I found this horrific truth, I stumbled upon [the kernel of XYZT](https://www.kaggle.com/thexyzt/keeping-it-simple-by-xyzt). It showed me how deep the rabbit hole goes."]},{"cell_type":"code","metadata":{"trusted":true,"_uuid":"6164a5eea32b8e101b9be0cefd02947fd7af50c8","_kg_hide-input":true,"id":"Iy_WoczcTXjY"},"source":["# sales_total_average = df_train['sales'].mean()\n","# df_train_norm = df_train.copy()\n","# df_train_norm['sales'] /= df_train.groupby(['item', 'store'])['sales'].transform('mean')\n","\n","fig, ax = plt.subplots(3, 2, figsize=(12, 16))\n","_ = df_train_norm.groupby(['item', 'Dayofweek'])['sales'].mean().unstack('item').plot(title='Weekly sales by item', legend=False, ax=ax[0,0])\n","_ = df_train_norm.groupby(['store', 'Dayofweek'])['sales'].mean().unstack('store').plot(title='Weekly sales by store', legend=False, ax=ax[1,0])\n","_ = df_train_norm.groupby(['item', 'store', 'Dayofweek'])['sales'].mean().unstack(['item', 'store']).plot(title='Weekly sales by item and store', legend=False, ax=ax[2,0])\n","_ = df_train_norm.groupby(['item', 'Year'])['sales'].mean().unstack('item').plot(title='Yearly sales by item', legend=False, ax=ax[0,1])\n","_ = df_train_norm.groupby(['store', 'Year'])['sales'].mean().unstack('store').plot(title='Yearly sales by store', legend=False, ax=ax[1,1])\n","_ = df_train_norm.groupby(['item', 'store', 'Year'])['sales'].mean().unstack(['item', 'store']).plot(title='Yearly sales by item and store', legend=False, ax=ax[2,1])\n","fig.suptitle('This is how deep the rabbit hole goes')\n","fig.tight_layout(rect=[0, 0, 1, 0.96])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"_uuid":"8bfab29fff878593fa72af9ce7b80a75b42cb03b","id":"GzmBK7uZTXjY"},"source":["## Math\n","\n","So, the data is not just synthetic. Basically, a sales volume time series for any item and store seems to be generated approximately as \n","\n","$TS(item, sales, date) = TS_{base}(date) * c(item, sales) * \\eta_1(item, sales, date) + \\eta_2(item, sales, date)$,\n","\n","where \n","- $TS_{base}$ is a fixed time series common for all items and stores,\n","- $c(item, sales)$ is a unique time series multiplier,\n","- $\\eta_1$ and $\\eta_2$ are white noise variables.\n","\n","Now we need to model $TS_{base}$. Judging by what we've seen so far, it can be expressed as\n","\n","$TS_{base}(date) = trend(year)*seasonality_{weekly}(weekday)*seasonality_{yearly}(month)$.\n","\n","Thus, to model sales volumes we need to estimate 4 functions: $c(item, sales)$, $trend(year)$, $seasonality_{weekly}(weekday)$ and $seasonality_{yearly}(month)$. Let's do it in the \"dumbest\" way possible: by averaging data.\n","\n","$c(item, sales) = average_{date} \\, TS(item, sales, date)$\n","\n","$trend(year) = \\frac{average_{item, store, year} \\, TS(item, sales, date)}{average_{total} \\, TS}$\n","\n","$seasonality_{weekly}(weekday) = \\frac{average_{item, store, weekday} \\, TS(item, sales, date)}{average_{total} \\, TS}$\n","\n","$seasonality_{yearly}(month) = \\frac{average_{item, store, month} \\, TS(item, sales, date)}{average_{total} \\, TS}$\n","\n","Now to make predictions, we simply look up values of $c$, $seasonality$ and $trend$. Okay, there is still one small issue: we have to extrapolate our trend.\n"]},{"cell_type":"markdown","metadata":{"_uuid":"64a3048fee844901cf53deed83e239adb1764d2d","id":"Yt84vpgBTXjY"},"source":["## Extrapolating trend\n","\n","Function extrapolation is never an easy task as even functions that have a similar fit on internal data points can have a very different behaviour outside of them. Check some examples below."]},{"cell_type":"code","metadata":{"trusted":true,"_uuid":"9607ab926bd265fdc51f2143e13ff0a234137a3e","_kg_hide-input":true,"scrolled":true,"id":"iW-7dP1ZTXjY"},"source":["annual_sales_avg = pd.pivot_table(df_trainext, values='sales', index='Year')\n","x_range = np.linspace(2013, 2018, 50)\n","annual_growth_linear = np.poly1d(np.polyfit(annual_sales_avg.index.values, annual_sales_avg['sales'].values, 1))\n","annual_growth_quadratic = np.poly1d(np.polyfit(annual_sales_avg.index.values, annual_sales_avg['sales'].values, 2))\n","annual_growth_linear_ll = (statsmodels.nonparametric\n","                           .kernel_regression.KernelReg(annual_sales_avg['sales'].values,\n","                                                        annual_sales_avg.index.values, \n","                                                        'c', bw=[1]))\n","_ = annual_sales_avg.plot(style='o', title=\"Average annual trend\", figsize=(8, 6))\n","_ = plt.plot(x_range, annual_growth_linear(x_range), '--', label='linear')\n","_ = plt.plot(x_range, annual_growth_linear_ll.fit(x_range)[0], '-.', label='local linear')\n","_ = plt.plot(x_range, annual_growth_quadratic(x_range), '.', label='quadratic')\n","_ = plt.legend()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"_uuid":"199aa788622487edc444215aac1171ca38055feb","id":"rWKb84ayTXjZ"},"source":["We could try using cross-validation to find the best extrapolation method. The problem is we only have 5 year to do so it is unlikely to be very efficient. When little data available stable methods usually perform the best: they have low variance and there is not much hope of having low bias anyway. So the best solution seems to be picking linear extrapolation or local linear extrapolation with high regularization. The solution with linear trend give us a public score of 13.875. Pretty good for such a \"dumb\" algorithm! This is also one of the earlier solutions described in XYZT's kernel.\n","\n","It's unfortunate that we don't have more data to tune the trend. I guess we should stop here. Unless we use one ancient dark and forbidden technique... We can tune the trend using the public score! Yes, it is typically better only to use it during final model validation rather than model parameter tuning - otherwise it's very easy overfit the model. However, in this case we should be fine: we are only tuning one parameter of a pretty stable model. This method gives us expected value of sales in 2018 approximately equal to 60.5, or $trend(2018) \\approx 1.158$. Let's just hardcode this value.\n","\n","The solution with a hardcoded trend give us a public score of 13.852 (and a private score of 12.587), which is pretty damn good!\n","\n","Just to be on the safe side, I cross-validated \"dumb\" models with linear, qudratic and hardcoded trend extrapolation, from 2016 Q1 to 2017 Q4 using the same methodology as before. The hardcoded version (HC) generally shows a lower error which is a pretty good sign."]},{"cell_type":"code","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true,"_uuid":"2260e02e1678bf885f4cab7c4bc0b35d0a9015c2","id":"jPyVtQJDTXjZ"},"source":["# Just for the reference, here is the code of the \"dumb\" model\n","from abc import abstractmethod, ABC\n","\n","class DumbBase(ABC):\n","    \"\"\"Dumb base model\"\"\"\n","    \n","    name = \"dumb_base\"\n","    \n","    def __init__(self, growth='linear', fit_window_years=None):\n","        self.growth = growth\n","        self.fit_window_years = fit_window_years\n","        self.verbose = True\n","    \n","    @staticmethod\n","    def expand_data(data):\n","        data_exp = data.copy()\n","        data_exp['day'] = data['date'].apply(lambda x: x.day)\n","        data_exp['month'] = data['date'].apply(lambda x: x.month)\n","        data_exp['year'] = data['date'].apply(lambda x: x.year)\n","        data_exp['dayofweek'] = data['date'].apply(lambda x: x.dayofweek)\n","        data_exp['weekofyear'] = data['date'].apply(lambda x: x.weekofyear)\n","        data_exp['dayofyear'] = data['date'].apply(lambda x: x.dayofyear)\n","        return data_exp\n","    \n","    def _fit_annual_sales(self):\n","        if isinstance(self.growth, pd.DataFrame):\n","            self._annual_sales = lambda x: self.growth.loc[x, 'sales']\n","        \n","        else:\n","            print('Dumb fit: functional annual growth')\n","            year_table = pd.pivot_table(self.data, index='year', values='sales', aggfunc=np.mean)\n","            years = year_table.index.values\n","            annual_sales_avg = year_table.values.squeeze()\n","\n","            if growth == 'linear': \n","                self._annual_sales = np.poly1d(np.polyfit(years, annual_sales_avg, 1))\n","            elif growth == 'quadratic': \n","                self._annual_sales = np.poly1d(np.polyfit(years, annual_sales_avg, 2))\n","            else:\n","                raise KeyError\n","    \n","    @abstractmethod\n","    def _fit_base_seasonality(self):\n","        pass\n","    \n","    def fit(self, data):\n","        if 'year' in data.columns:\n","            self.data = data.copy()\n","        else:\n","            print('Dumb fit: Expand data')\n","            self.data = self.expand_data(data)\n","        \n","        if self.fit_window_years is not None:\n","            date_max = self.data['date'].max()\n","            date_min = date_max.replace(year=date_max.year-self.fit_window_years)\n","            self.data = self.data.query('date > @date_min')\n","            \n","        self.data['sales'] /= self.data['sales'].mean()\n","        self._fit_base_seasonality()\n","        self._fit_annual_sales()        \n","        \n","    @abstractmethod\n","    def _predict_base_seasonality(self, item, store, date):\n","        pass\n","    \n","    def _predict_annual_sales(self, year):\n","        return self._annual_sales(year)\n","        \n","    def predict(self, data):\n","        data = data.assign(sales_hat=.001)\n","        with suppress_stdout(not self.verbose):\n","            timer = timer_gen()\n","            count = 1\n","            for i, row in data.iterrows():\n","                if count % 100000 == 0: print(\"dumb predict {} {}\".format(count, next(timer), end=' | '))\n","                date, item, store = row['date'], row['item'], row['store']\n","                pred_sales = self._predict_base_seasonality(item, store, date) * self._predict_annual_sales(date.year)\n","                data.at[i, 'sales_hat'] = pred_sales\n","                count += 1\n","        return data\n","    \n","    \n","class DumbOriginal(DumbBase):\n","    \"\"\"\n","    Original Dumb model\n","    \n","    sales = base(item, store) * s(dayofweek) * s(month) * trend(year)\n","    \"\"\"\n","    \n","    name = \"dumb_original\"\n","    \n","    def _fit_base_seasonality(self):\n","        self.store_item_table = pd.pivot_table(self.data, index='store', columns='item',\n","                                               values='sales', aggfunc=np.mean)\n","        self.month_table = pd.pivot_table(self.data, index='month', values='sales', aggfunc=np.mean)\n","        self.dow_table = pd.pivot_table(self.data, index='dayofweek', values='sales', aggfunc=np.mean)\n","        \n","    def _predict_base_seasonality(self, item, store, date):\n","        dow, month, year = date.dayofweek, date.month, date.year\n","        base_sales = self.store_item_table.at[store, item]\n","        seasonal_sales = self.month_table.at[month, 'sales'] * self.dow_table.at[dow, 'sales']\n","        return base_sales * seasonal_sales    "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"_kg_hide-input":true,"trusted":true,"scrolled":false,"_uuid":"7462ec3f9634b6316c2235eb10242ce77d5a5381","_kg_hide-output":true,"id":"GivCaAhQTXjZ"},"source":["dumb_hc_cv = pd.read_csv('../input/sales-dumb-all-cv2017/cv_dumb_hc_oos_yk.csv', index_col=[0,1,2,3])\n","print('N rows = ', dumb_hc_cv.shape[0])\n","display(dumb_hc_cv.head(3))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"trusted":true,"_uuid":"2ca822cdc01492e64d47ba5ba0f12302e04384ab","_kg_hide-input":true,"scrolled":false,"_kg_hide-output":true,"id":"Ok0UgDCpTXjZ"},"source":["dumb_linear_cv = pd.read_csv('../input/sales-dumb-all-cv2017/cv_dumb_linear_oos_yk.csv', index_col=[0,1,2,3])\n","display(dumb_linear_cv.head(3))\n","dumb_quadratic_cv = pd.read_csv('../input/sales-dumb-all-cv2017/cv_dumb_quadratic_oos_yk.csv', index_col=[0,1,2,3])\n","display(dumb_quadratic_cv.head(3))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"24c2260b4e0a2fab01a440b71b9ea5f7f2394ebd","id":"wzDycQ4GTXja"},"source":["def create_df_cv_smape_arrg(df_cv_dict):\n","    df_smape_aggr = pd.concat((val.groupby(['cv_fold']).apply(smape_df).rename(key)\n","                               for key, val in df_cv_dict.items()), axis=1)\n","    df_smape_aggr.index = [x - pd.Timedelta(days=1) for x in pd.to_datetime(df_smape_aggr.index)]\n","    return df_smape_aggr\n","    \n","df_dumb_smape_aggr = create_df_cv_smape_arrg(OrderedDict((\n","    ('Dumb HC', dumb_hc_cv),\n","    ('Dumb Linear', dumb_linear_cv),\n","    ('Dumb Quadratic', dumb_quadratic_cv),\n",")))\n","_ = df_dumb_smape_aggr.plot(figsize=(10, 3), style='o--', title='Dumb CV SMAPE')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"_uuid":"72c33c853ef47eca5c86cc3efcfcfbabc59b8ee8","id":"glUOY5CgTXja"},"source":["To be on a even safer side, let's check the cross-validated errors date by date. *Signed* SMAPE is SMAPE without absolute operator:\n","\n","$$SMAPE_{signed} = average_i \\frac{y_i-\\hat{y_i}}{\\frac{1}{2}(y_i+\\hat{y_i})} \\cdot 100\\%.$$\n","\n","If our model is not strongly biased, signed SMAPE should be close to zero. \n","\n","The hardcoded version seems to perform alright. Awesome!"]},{"cell_type":"code","metadata":{"_kg_hide-input":true,"trusted":true,"scrolled":true,"_uuid":"7d6999304d4931b7909abc081616b1fa9f626cbb","id":"lT7XAOVyTXja"},"source":["def show_cv_smape_by_time_oos_2(data_cv, m_name):\n","    data_gb = data_cv.groupby(['cv_fold', 'ds'])\n","    df_u = data_gb.apply(smape_df, signed=False).unstack(level='cv_fold')\n","    df_s = data_gb.apply(smape_df, signed=True).unstack(level='cv_fold')\n","    cols_original = df_u.columns.copy()\n","    df_u.columns = [x + '_u' for x in cols_original]\n","    df_s.columns = [x + '_s' for x in cols_original]\n","    df = pd.concat([df_u, df_s], axis=1)\n","    for col in df.columns:\n","        df[col] = lowess(df[col], range(df.shape[0]), frac=0.15, return_sorted=False)\n","    df.index = pd.to_datetime(df.index)\n","\n","    source = ColumnDataSource(df)\n","    p_u = figure(plot_width=750, plot_height=200, title=\"{} OOS SMAPE by time\".format(m_name),\n","                 x_axis_type='datetime', tools=\"pan,wheel_zoom,reset\")\n","    for col in df_u.columns:\n","        _ = p_u.line(x='ds', y=col, source=source, color='red')\n","        \n","    p_s = figure(plot_width=750, plot_height=200, title=\"{} OOS SMAPE by time (signed)\".format(m_name),\n","                 x_axis_type='datetime', tools=\"pan,wheel_zoom,reset\", x_range=p_u.x_range)\n","    for col in df_s.columns:\n","        _ = p_s.line(x='ds', y=col, source=source, color='blue')\n","        \n","    for col in np.append(cols_original.values, \"2016-01-01\"):\n","        p_s.add_layout(Span(location=pd.to_datetime(col).value / 1e6,\n","                            dimension='height', line_color='black',\n","                            line_dash='dashed', line_width=1))\n","        p_u.add_layout(Span(location=pd.to_datetime(col).value / 1e6,\n","                            dimension='height', line_color='black',\n","                            line_dash='dashed', line_width=1))\n","    p_s.add_tools(HoverTool(tooltips=[('smape', '$y')], mode='vline', line_policy='interp'))    \n","    p_u.add_tools(HoverTool(tooltips=[('smape', '$y')], mode='vline', line_policy='interp'))\n","    \n","    bokeh.io.show(bokeh.layouts.column(p_u, p_s))\n","    \n","show_cv_smape_by_time_oos_2(dumb_hc_cv, 'Dumb HC')\n","show_cv_smape_by_time_oos_2(dumb_linear_cv, 'Dumb Linear')\n","show_cv_smape_by_time_oos_2(dumb_quadratic_cv, 'Dumb Quadratic')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"_uuid":"9a373adfdb728bb62017442efcfdaff09f51b6f0","id":"95mHOVNITXja"},"source":["Like before, we can check best and worst performers across items and stores. Okay, I don't see any obvious pattern here and the time is limited... Let's move on!"]},{"cell_type":"code","metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"3eed47e55f90e6f73ad37e3007c8c712eb413658","scrolled":false,"id":"JDlzvnIlTXja"},"source":["def plot_data_cv_example_2(df, item, store, ax):\n","    ts = df.loc[(slice(None), slice(None), item, store), ['y', 'yhat']].reset_index(level=[2,3], drop=True).unstack(level=['cv_fold'])\n","    ts.index = pd.to_datetime(ts.index)\n","    (ts['yhat'] - ts['y']).abs().resample('w').mean().plot(ax=ax, title=\"{}_{}\".format(item, store), legend=False)\n","\n","def show_cv_best_worst_2(data_cv, cutoff_eval):\n","    smape_itsa = data_cv.groupby(['item', 'store']).apply(smape_df).sort_values()\n","    print(\"-------- Best: Lowest SMAPE OOS ---------\")\n","    display_side_by_side(smape_itsa.head())\n","    print(\"-------- Worst: Highest SMAPE OOS ---------\")\n","    display_side_by_side(smape_itsa.tail())\n","\n","    cv_fold_idx = cutoff_eval\n","    fig, ax = plt.subplots(3, 1, figsize=(12, 6))\n","    plot_data_cv_example_2(df=data_cv, item=smape_itsa.index[0][0], store=smape_itsa.index[0][1], ax=ax[0])\n","    plot_data_cv_example_2(df=data_cv, item=smape_itsa.index[1][0], store=smape_itsa.index[1][1], ax=ax[1])\n","    plot_data_cv_example_2(df=data_cv, item=smape_itsa.index[2][0], store=smape_itsa.index[2][1], ax=ax[2])\n","    fig.suptitle('Best: Lowest OOS SMAPE')\n","    fig.tight_layout(rect=[0, 0, 1, 0.94])\n","\n","    fig, ax = plt.subplots(3, 1, figsize=(12, 6))\n","    plot_data_cv_example_2(df=data_cv, item=smape_itsa.index[-1][0], store=smape_itsa.index[-1][1], ax=ax[0])\n","    plot_data_cv_example_2(df=data_cv, item=smape_itsa.index[-2][0], store=smape_itsa.index[-2][1], ax=ax[1])\n","    plot_data_cv_example_2(df=data_cv, item=smape_itsa.index[-3][0], store=smape_itsa.index[-3][1], ax=ax[2])\n","    fig.suptitle('Worst: Highest OOS SMAPE')\n","    fig.tight_layout(rect=[0, 0, 1, 0.94])\n","    \n","show_cv_best_worst_2(dumb_hc_cv, cutoff_eval=\"2017-04-01\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"_uuid":"e42d0e6dbc09e124a3658d83a4aadf24f8fb80a7","id":"eJJTrEhNTXjb"},"source":["## Tuning seasonality by CV\n","\n","The \"dumb\" model works pretty well already. However, this model is quite rigid so it could benifit from adding some flexibility. There are a few ways to do it:\n","- Custom seasonalities: fit seasonalities individually for different stores or items;\n","- More granular seasonality: e.g., add monthly seasonality;\n","- Fit the model using recent data only;\n","- Fit another model on residuals: e.g., Prophet;\n","- Blend with other models: e.g., with Boosted Trees.\n","\n","I've tried all these approaches. Some of them did alright and may have been used in our final submission if we had more time; others less so. In this kernel we'll focus on the very first approach."]},{"cell_type":"markdown","metadata":{"_uuid":"2739ec71077577a2f51ac9d2a4d0acca32f96619","id":"FRV0mq0xTXjb"},"source":["## Custom item-store weekly seasonalities\n","\n","To begin, let's try fitting weekly seasonalities separately for every store, item or both. For example, if we fit sesonalities separately by item, $seasonality_{weekly}(weekday)$ becomes $seasonality_{item, weekly}(item, weekday)$. \n"," \n","Great, there is a slight improvement for seasonalities fit by item and seasonalities fit by store! If we try fitting seasonalities by item and store at once though, our model seems to overfit."]},{"cell_type":"code","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true,"_uuid":"dab084c3ecc2226e64bbb85109400e8276f56f9d","id":"laf9ME1yTXjb"},"source":["# Just for the reference, here is the code of the seasonal variations\n","\n","class DumbItemDayofweek(DumbBase):\n","    \"\"\"\n","    sales = base(store) * s(dayofweek, item) * s(month) * trend(year)\n","    \"\"\"\n","    \n","    name = \"dumb_item_dow\"\n","    \n","    def _fit_base_seasonality(self):\n","        self.store_table = pd.pivot_table(self.data, index='store', values='sales', aggfunc=np.mean)\n","\n","        self.month_table = pd.pivot_table(self.data, index='month', values='sales', aggfunc=np.mean)\n","\n","        self.dow_item_table = pd.pivot_table(self.data, index='dayofweek', columns='item',\n","                                             values='sales', aggfunc=np.mean)\n","        \n","    def _predict_base_seasonality(self, item, store, date):\n","        dow, month, year = date.dayofweek, date.month, date.year\n","        base_sales = self.store_table.at[store, 'sales']\n","        seasonal_sales = self.month_table.at[month, 'sales'] * self.dow_item_table.at[dow, item]\n","        return base_sales * seasonal_sales\n","    \n","    \n","class DumbStoreDayofweek(DumbBase):\n","    \"\"\"\n","    sales = base(item) * s(dayofweek, store) * s(month) * trend(year)\n","    \"\"\"\n","    \n","    name = \"dumb_store_dow\"\n","    \n","    def _fit_base_seasonality(self):\n","        self.item_table = pd.pivot_table(self.data, index='item', values='sales', aggfunc=np.mean)\n","\n","        self.month_table = pd.pivot_table(self.data, index='month', values='sales', aggfunc=np.mean)\n","\n","        self.dow_store_table = pd.pivot_table(self.data, index='dayofweek', columns='store',\n","                                             values='sales', aggfunc=np.mean)\n","        \n","    def _predict_base_seasonality(self, item, store, date):\n","        dow, month, year = date.dayofweek, date.month, date.year\n","        base_sales = self.item_table.at[item, 'sales']\n","        seasonal_sales = self.month_table.at[month, 'sales'] * self.dow_store_table.at[dow, store]\n","        return base_sales * seasonal_sales\n","    \n","    \n","class DumbItemStoreDayofweek(DumbBase):\n","    \"\"\"\n","    sales = base() * s(dayofweek, item, store) * s(month) * trend(year)\n","    \"\"\"\n","    \n","    name = \"dumb_item_store_dow\"\n","    \n","    def _fit_base_seasonality(self):\n","        self.base_scalar = 1.\n","\n","        self.month_table = pd.pivot_table(self.data, index='month', values='sales', aggfunc=np.mean)\n","\n","        self.dow_item_store_table = pd.pivot_table(self.data, index=['item', 'store'],\n","                                                   columns='dayofweek',\n","                                                   values='sales', aggfunc=np.mean)\n","        \n","    def _predict_base_seasonality(self, item, store, date):\n","        dow, month, year = date.dayofweek, date.month, date.year\n","        base_sales = self.base_scalar\n","        seasonal_sales = self.month_table.at[month, 'sales'] * self.dow_item_store_table.at[(item, store), dow]\n","        return base_sales * seasonal_sales"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"trusted":true,"_uuid":"a92de3f4201d144117042b6bf813c7d9385329af","_kg_hide-input":true,"_kg_hide-output":true,"id":"p9a3rvfNTXjc"},"source":["# --- Tweaking item-store dependencies\n","\n","dumb_original_cv = pd.read_csv('../input/sales-dumb-all-alt-cv2017/cv_dumb_original.csv', index_col=[0,1,2,3])\n","dumb_item_dow_cv = pd.read_csv('../input/sales-dumb-all-alt-cv2017/cv_dumb_item_dow.csv', index_col=[0,1,2,3])\n","dumb_store_dow_cv = pd.read_csv('../input/sales-dumb-all-alt-cv2017/cv_dumb_store_dow.csv', index_col=[0,1,2,3])\n","dumb_item_store_dow_cv = pd.read_csv('../input/sales-dumb-all-alt-cv2017/cv_dumb_item_store_dow.csv', index_col=[0,1,2,3])\n","print('N rows = ', dumb_original_cv.shape[0])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"trusted":true,"scrolled":true,"_uuid":"7a66aa11b2a8297e31114ab23a9a64d6ba60386b","_kg_hide-input":true,"id":"tcun6pupTXjc"},"source":["df_dumb_smape_aggr = create_df_cv_smape_arrg(OrderedDict((\n","    ('Dumb HC Original', dumb_original_cv),\n","    ('Dumb HC Item DOW', dumb_item_dow_cv),\n","    ('Dumb HC Store DOW', dumb_store_dow_cv),\n","    ('Dumb HC Item-Store DOW', dumb_item_store_dow_cv),\n",")))\n","_ = df_dumb_smape_aggr.plot(figsize=(10, 3), style='o--', title='Dumb HC CV SMAPE')\n","\n","df_dumb_smape_aggr['Dumb HC Item DOW'] -= df_dumb_smape_aggr['Dumb HC Original']\n","df_dumb_smape_aggr['Dumb HC Store DOW'] -= df_dumb_smape_aggr['Dumb HC Original']\n","df_dumb_smape_aggr['Dumb HC Item-Store DOW'] -= df_dumb_smape_aggr['Dumb HC Original']\n","df_dumb_smape_aggr.drop(columns=['Dumb HC Original'], inplace=True)\n","_ = df_dumb_smape_aggr.plot(figsize=(10, 3), style='o--', title='Difference with Dumb HC Original')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"trusted":true,"scrolled":true,"_uuid":"d554ca3f248774d667f2befe87114c57606c6d68","id":"TtCEm9WSTXjc"},"source":["show_cv_smape_by_time_oos_2(dumb_original_cv, 'Dumb HC Original')\n","show_cv_smape_by_time_oos_2(dumb_item_dow_cv, 'Dumb HC Item DOW')\n","show_cv_smape_by_time_oos_2(dumb_store_dow_cv, 'Dumb HC Store DOW')\n","show_cv_smape_by_time_oos_2(dumb_item_store_dow_cv, 'Dumb HC Item-Store DOW')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"_uuid":"8aa37a34a93a0267ecc40495c3e89cfe17261825","id":"XRUQbsgZTXjc"},"source":["<a id=\"SectionOutro\"></a>\n","# Outro\n","\n","For our final submission, we selected the original \"dumb\" model (with a hardcoded trend) and the \"dumb\" model with weekly seasonalities fit for every item separately. The latter had a low and stable cross-validated error and a pretty good public score of 13.845. This choice has payed off: the model got a private score of 12.580 and brought our team to Top 1! Yay! It was quite close to the second best private score of 12.584 by jnng. \n","\n","Going through this kernel again, I think the key points contribusing to *Fantastic 2*'s victory in this competions were:\n","- Realization that the data is synthetic;\n","- Ideas inspired by other public kernels;\n","- Careful use of a public score to tune the trend;\n","- Cross-validation to select the best model;\n","- Luck =)\n","\n","On ideas from public kernels: [XYZT's kernel](https://www.kaggle.com/thexyzt/keeping-it-simple-by-xyzt) made a big difference. To be honest, in this era of Gradient Boosting and DNN, I'm not sure we would have thought of such a simple approach to model sales volumes. Simple yet very effective! The leaderboard changed drastically after XYZT shared it - it was very funny to see how at least a dozen top submissions appreared by simply forking his script ^^\n","\n","And... that's it folks! Thanks for reading. Please share your thoughts in the comment section. And cross-validate responsibly."]},{"cell_type":"markdown","metadata":{"_uuid":"9ef0eb400cb692a9498daf96d9b8dba557bc6568","id":"beGstfqtTXjd"},"source":["> \\---------------------------\n",">\n","> Stay awesome Kaggle,\n",">\n","> Mysterious Ben\n",">\n","> \\---------------------------"]},{"cell_type":"code","metadata":{"trusted":true,"_uuid":"d34d96d48154fe2e6cc59b72832f06075f25add9","id":"3tyo-SrrTXjd"},"source":[""],"execution_count":null,"outputs":[]}]}